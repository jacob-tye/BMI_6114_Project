# BMI_6114_Project
## ðŸ§¬ Predicting Survival Time in Lung Cancer (BMI 6114 Final Project)
This project explores the application of traditional machine learning and deep learning models to predict survival time in lung cancer patients. Using clinical and genomic data from TCGA, we implemented multiple pipelines, including feature preprocessing, dimensionality reduction with autoencoders, and regression modeling using Random Forests, Gradient Boosting, and deep neural networks.

## ðŸ—‚ Project Structure
```
â”œâ”€â”€ README.md
â””â”€â”€ analysis/
    â”œâ”€â”€ 00_data_prep.ipynb              # Preprocessing and feature engineering
    â”œâ”€â”€ 01_ml_analysis_gbm.ipynb        # Gradient Boosting Models (unscaled and scaled)
    â”œâ”€â”€ 01_ml_analysis_rf.ipynb         # Random Forest Regressor Models (unscaled)
    â”œâ”€â”€ 01_ml_analysis_rf_scaled.ipynb         # Random Forest Regressor Models (scaled)
    â”œâ”€â”€ 02_autoencoder.ipynb            # Autoencoder for dimensionality reduction
    â”œâ”€â”€ 03_dl_analysis.ipynb            # Deep learning with autoencoder (unscaled)
    â”œâ”€â”€ 04_dl_analysis_no_ae.ipynb      # Deep learning without autoencoder (unscaled)
    â”œâ”€â”€ 05_dl_analysis_scaled.ipynb     # Deep learning with autoencoder (log-scaled target)
    â”œâ”€â”€ 06_dl_analysis_no_ae_scaled.ipynb # Deep learning without autoencoder (log-scaled)
    â”œâ”€â”€ results/                        # Best models and paremeters for use in later steps
    â”œâ”€â”€ optuna_logs/                    # Optuna logs and checkpoints for hyperparameter tuning
    â””â”€â”€ logs/                           # Lightning logs and checkpoints for all model trials
```
## ðŸ§ª Methods Overview
### Data Source
TCGA LUAD and LUSC samples

Downloaded and filtered via [cBioPortal](https://www.cbioportal.org/study/summary?id=nsclc_tcga_broad_2016)

Features Used
Smoking history and pack years (clinical)

Mutation and copy number variation (genomic)

One-hot encoded + standardized features

### Modeling Approaches

Category	Models Implemented	Notes
ML	Random Forest, Gradient Boosting	Hyperparameter tuning via Optuna
Dimensionality	Autoencoder	Latent compression to reduce feature space
DL	Feedforward Neural Networks	With/without autoencoder; raw vs log targets
Experiment Logs	PyTorch Lightning logging to logs/	Models saved with metrics.csv and hparams.yaml per run

### ðŸ“Š Results Snapshot
ðŸŸ¢ Best ML Model: Random Forest on unscaled targets (RÂ² = 0.416)

ðŸ”´ Deep Learning Models: All had RÂ² < 0 on test set due to overfitting and dataset size

ðŸ” Overfitting Test: Scaled NN models showed learnability on training set with R â‰ˆ 0.78

ðŸ“„ Report
A full write-up of our methods, results, limitations, and visualizations is available in final_report.docx.
